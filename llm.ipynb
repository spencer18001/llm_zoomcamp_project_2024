{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install semchunk tiktoken sentence_transformers elasticsearch openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "import requests\n",
    "import functools\n",
    "from tqdm.auto import tqdm\n",
    "import semchunk\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52991"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path = 'data/The_Adventure_of_the_Speckled_Band.txt'\n",
    "with open(data_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "chunker = semchunk.chunkerify(tiktoken.encoding_for_model('gpt-4o'), chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunker(content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'all-mpnet-base-v2'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = embedding_model.get_sentence_embedding_dimension()\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What were the circumstances that led to the death of Julia Stoner?\"\n",
    "v = embedding_model.encode(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:18<00:00,  8.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "for chunk in tqdm(chunks):\n",
    "    doc = {\n",
    "        'text': chunk,\n",
    "        'vector': embedding_model.encode(chunk),\n",
    "    }\n",
    "    docs.append(doc)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_elasticsearch = 'http://localhost:9200'\n",
    "\n",
    "def create_elasticsearch_client():\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url_elasticsearch)\n",
    "        except requests.ConnectionError:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            break\n",
    "    client = Elasticsearch(url_elasticsearch)\n",
    "    print(json.dumps(client.info().raw, indent=4))\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"3d10f11edd60\",\n",
      "    \"cluster_name\": \"docker-cluster\",\n",
      "    \"cluster_uuid\": \"t-WOVq08TcelO6DwA352WQ\",\n",
      "    \"version\": {\n",
      "        \"number\": \"8.9.0\",\n",
      "        \"build_flavor\": \"default\",\n",
      "        \"build_type\": \"docker\",\n",
      "        \"build_hash\": \"8aa461beb06aa0417a231c345a1b8c38fb498a0d\",\n",
      "        \"build_date\": \"2023-07-19T14:43:58.555259655Z\",\n",
      "        \"build_snapshot\": false,\n",
      "        \"lucene_version\": \"9.7.0\",\n",
      "        \"minimum_wire_compatibility_version\": \"7.17.0\",\n",
      "        \"minimum_index_compatibility_version\": \"7.0.0\"\n",
      "    },\n",
      "    \"tagline\": \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "es_client = create_elasticsearch_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'story_chunks'})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": embedding_size,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"story_chunks\"\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:03<00:00, 44.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(docs):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elasticsearch_knn(field, vector):\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"text\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "\n",
    "    result_docs = []\n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\"You can at least tell me whether my own thought is correct, and if she died from some sudden fright.\"\\n\"No, I do not think so. I think that there was probably some more tangible cause. And now, Miss Stoner, we must '},\n",
       " {'text': '\"I had come to these conclusions before ever I had entered his room. An inspection of his chair showed me that he had been in the habit of standing on it, which of course would be necessary in order that he should reach the ventilator. The sight of the safe, the saucer of milk, and the loop of whipcord were enough to finally dispel any doubts which may have remained. The metallic clang heard by Miss Stoner was obviously caused by her step-father hastily closing the'},\n",
       " {'text': '\"She died just two years ago, and it is of her death that I wish to speak to you. You can understand that, living the life which I have described, we were little likely to see anyone of our own age and position. We had, however, an aunt, my mother\\'s maiden sister. Miss Honoria Westphail, who lives near Harrow, and we were occasionally allowed to pay short visits at this lady\\'s house. Julia went there at Christmas two years ago, and'},\n",
       " {'text': '\"Oh yes, easily.\"\\n\\n\"The rest you will leave in our hands.\"\\n\\n\"But what will you do?\"\\n\\n\"We shall spend the night in your room, and we shall investigate the cause of this noise which has disturbed you.\"\\n\\n\"I believe, Mr. Holmes, that you have already made up your mind,\" said Miss Stoner, laying her hand upon my companion\\'s sleeve.\\n\\n\"Perhaps I have.\"\\n\\n\"Then for pity\\'s sake tell me what was the cause of my sister\\'s death.\"'},\n",
       " {'text': '\"You can imagine from what I say that my poor sister Julia and I had no great pleasure in our lives. No servant would stay with us, and for a long time we did all the work of the house. She was but thirty at the time of her death, and yet her hair had already begun to whiten, even as mine has.\"\\n\\n\"Your sister is dead, then?\"'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = elasticsearch_knn('vector', v)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ollama = 'http://localhost:11434'\n",
    "\n",
    "def create_ollama_client(model):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url_ollama)\n",
    "        except requests.ConnectionError:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(response.content.decode())\n",
    "            break\n",
    "    \n",
    "    response = requests.post(f'{url_ollama}/api/pull', json={\"name\": model})\n",
    "    print(response.status_code)\n",
    "    print(response)\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(f'{url_ollama}/api/tags')\n",
    "        if len(response.json().get(\"models\", [])) > 0:\n",
    "            print(json.dumps(response.json(), indent=4))\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "    api_key = \"ollama\"\n",
    "    base_url = f\"{url_ollama}/v1/\"\n",
    "    return OpenAI(api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n",
      "200\n",
      "<Response [200]>\n",
      "{\n",
      "    \"models\": [\n",
      "        {\n",
      "            \"name\": \"phi3:latest\",\n",
      "            \"model\": \"phi3:latest\",\n",
      "            \"modified_at\": \"2024-08-27T04:57:33.027550777Z\",\n",
      "            \"size\": 2176178913,\n",
      "            \"digest\": \"4f222292793889a9a40a020799cfd28d53f3e01af25d48e06c5e708610fc47e9\",\n",
      "            \"details\": {\n",
      "                \"parent_model\": \"\",\n",
      "                \"format\": \"gguf\",\n",
      "                \"family\": \"phi3\",\n",
      "                \"families\": [\n",
      "                    \"phi3\"\n",
      "                ],\n",
      "                \"parameter_size\": \"3.8B\",\n",
      "                \"quantization_level\": \"Q4_0\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'phi3'\n",
    "ol_client = create_ollama_client(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert detective analyzing the details of the story \"The Adventure of the Speckled Band.\" Answer the QUESTION using only the relevant information provided in the CONTEXT from the story.\n",
    "\n",
    "Make sure to stay true to the facts in the CONTEXT when answering the QUESTION. Avoid adding any outside knowledge or assumptions.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context = context + f\"text: {doc['text']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ollama(client, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(search_func, llm_func, build_prompt_func, query, query_vec):\n",
    "    search_results = search_func(query_vec)\n",
    "    prompt = build_prompt_func(query, search_results)\n",
    "    answer = llm_func(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2319"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(build_prompt(question, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- Julia Stoner died two years ago after accidentally inhaling a venomous snake that Dr. Jekyll's step-father drugged the snake with for entertainment purposes to poison her without raising suspicion since they had no servants and their family members were also too young or old, as suggested by Mrs. Stoner’s description of Julia'0–3 being an uncommon age in that setting given social prejudices against youthful females lacking employable experience due to minimal working-age population mentioned implicitly near the beginning/end where no one from their own generation frequently encountered each other, adding layers such as her habitual standing on a chair and routine objects within reach further point towards Dr. Jekyll's manipulative nature exploiting these familial weaknesses intentionally without any mention of personal psychological issues contributing to self-harm or suicide implying external danger leading directly to accidental death, all inferred from the interlocked details shared concerning Julia and Miss Stoner with no direct involvement by either as narrating detectives.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\n",
    "    search_func=functools.partial(elasticsearch_knn, 'vector'),\n",
    "    llm_func=functools.partial(llm_ollama, ol_client),\n",
    "    build_prompt_func=build_prompt,\n",
    "    query=question,\n",
    "    query_vec=v\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "45e0acf3aaa0c86bae9b8152225e6af3a989c01d4699619c71c813825d4fb049"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
